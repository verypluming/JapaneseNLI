{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/verypluming/JapaneseNLI/blob/master/JapaneseXLM_NLI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oidzllMksJWm"
   },
   "source": [
    "transformersライブラリのXLM: https://github.com/huggingface/transformers をファインチューニングして日本語テキスト推論を試すコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpoDfZd7TNNu"
   },
   "outputs": [],
   "source": [
    "# 必要なモジュールのインストール\n",
    "! pip install transformers==2.6.0 mecab-python3==0.996.5 tensorflow scikit-learn pandas lxml\n",
    "%tensorflow_version 2.x. \n",
    "!mkdir data\n",
    "!mkdir models\n",
    "# Google Driveのマウント\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnaqlPeLWrj0"
   },
   "outputs": [],
   "source": [
    "# 必要なモジュール・関数の読み込み\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import XLMConfig, TFXLMForSequenceClassification, XLMTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "class Vocab:\n",
    "    # 正解ラベルの設定（今回はcontradiction, entailment, neutralの３値を設定）\n",
    "    def __init__(self):\n",
    "        self.token_index = {label: i for i, label in enumerate(set([\"contradiction\", \"entailment\", \"neutral\"]))}\n",
    "        self.index_token = {v: k for k, v in self.token_index.items()}\n",
    "\n",
    "    def encode(self, labels):\n",
    "        label_ids = [self.token_index.get(label) for label in labels]\n",
    "        return label_ids\n",
    "\n",
    "    def decode(self, label_ids):\n",
    "        labels = [self.index_token.get(label_id) for label_id in label_ids]\n",
    "        return labels\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.token_index)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            config = {\n",
    "                'token_index': self.token_index,\n",
    "                'index_token': self.index_token\n",
    "            }\n",
    "            f.write(json.dumps(config))\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file_path):\n",
    "        with open(file_path) as f:\n",
    "            config = json.load(f)\n",
    "            vocab = cls()\n",
    "            vocab.token_index = config.token_index\n",
    "            vocab.index_token = config.index_token\n",
    "        return vocab\n",
    "\n",
    "def convert_examples_to_features(x, y, vocab, max_seq_length, tokenizer):\n",
    "    features = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'token_type_ids': [],\n",
    "        'label_ids': np.asarray(vocab.encode(y))\n",
    "    }\n",
    "    for pairs in x:\n",
    "        tokens = [tokenizer.cls_token]\n",
    "        token_type_ids = []\n",
    "        for i, sent in enumerate(pairs):\n",
    "            word_tokens = tokenizer.tokenize(sent)\n",
    "            tokens.extend(word_tokens)\n",
    "            tokens += [tokenizer.sep_token]\n",
    "            len_sent = len(word_tokens) + 1\n",
    "            token_type_ids += [i] * len_sent\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        features['input_ids'].append(input_ids)\n",
    "        features['attention_mask'].append(attention_mask)\n",
    "        features['token_type_ids'].append(token_type_ids)\n",
    "\n",
    "    for name in ['input_ids', 'attention_mask', 'token_type_ids']:\n",
    "        features[name] = pad_sequences(features[name], padding='post', maxlen=max_seq_length)\n",
    "\n",
    "    x = [features['input_ids'], features['attention_mask'], features['token_type_ids']]\n",
    "    y = features['label_ids']\n",
    "    return x, y\n",
    "\n",
    "def build_model(pretrained_model_name_or_path, num_labels):\n",
    "    config = XLMConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    model = TFXLMForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        config=config\n",
    "    )\n",
    "    model.layers[-1].activation = tf.keras.activations.softmax\n",
    "    return model\n",
    "\n",
    "def evaluate(model, target_vocab, features, labels):\n",
    "    label_ids = model.predict(features)\n",
    "    label_ids = np.argmax(label_ids, axis=-1)\n",
    "    y_pred = target_vocab.decode(label_ids)\n",
    "    y_true = target_vocab.decode(labels)\n",
    "    print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baN8z3fCfLjK"
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "model_path = 'models/'\n",
    "pretrained_model_name_or_path = 'xlm-mlm-100-1280'\n",
    "tokenizer = XLMTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "maxlen = 250\n",
    "target_vocab =Vocab()\n",
    "\n",
    "# モデルの構築\n",
    "model = build_model(pretrained_model_name_or_path, target_vocab.size)\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# callbacksの設定\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQ53YL45VaQm"
   },
   "outputs": [],
   "source": [
    "# ファインチューニングに用いるデータの読み込み\n",
    "# 各行にタブ区切りでpremise（前提文）、hypothesis(仮説文)、gold_label（正解ラベル）が書かれたtrain.tsvファイルを用意し、Google Driveにアップロード\n",
    "# （一行目はpremise, hypothesis, gold_labelと記述）\n",
    "# データの例\n",
    "# premise hypothesis  gold_label\n",
    "# 太郎は花子が山頂まで登っている間に、山頂まで登った。  太郎は花子が山頂まで登る前に、山頂まで登った。  entailment\n",
    "!cp /content/drive/My\\ Drive/train.tsv data/.\n",
    "df = pd.read_csv(\"data/train.tsv\", sep=\"\\t\")\n",
    "premises = list(df['premise'])\n",
    "hypotheses = list(df['hypothesis'])\n",
    "x = [(premise, hypothesis) for (premise, hypothesis) in zip(premises, hypotheses)]\n",
    "y = list(df['gold_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ci94_l0BsIVI"
   },
   "outputs": [],
   "source": [
    "# 全データをファインチューニングに使う場合\n",
    "x_train = x\n",
    "y_train = y\n",
    "# train:testを9:1で分割して評価する場合\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "features_train, labels_train = convert_examples_to_features(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    target_vocab,\n",
    "    max_seq_length=maxlen,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# モデルのファインチューニング\n",
    "model.fit(x=features_train,\n",
    "          y=labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1,\n",
    "          callbacks=callbacks)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWanq1vpW2Jy"
   },
   "outputs": [],
   "source": [
    "# ファインチューニングしたモデルをMyDriveに保存\n",
    "!cp models/tf_model.h5 /content/drive/My\\ Drive/.\n",
    "!cp models/config.json /content/drive/My\\ Drive/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywLq9ejNW4Dt"
   },
   "outputs": [],
   "source": [
    "# MyDriveに保存したモデルの読み込み（モデルがある場合に使用）\n",
    "!cp /content/drive/My\\ Drive/tf_model.h5 models/.\n",
    "!cp /content/drive/My\\ Drive/config.json models/.\n",
    "config = XLMConfig.from_json_file('models/config.json')\n",
    "model = TFXLMForSequenceClassification.from_pretrained('models/tf_model.h5', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLcAGFXJdRQ5"
   },
   "outputs": [],
   "source": [
    "# 任意のデータでテスト\n",
    "# x_test:　前提文と仮説文のペアをタプルとしたリスト, y_test: 正解ラベル（entailmentかneutralかcontradiction）のリスト\n",
    "x_test = [('太郎は花子が山頂まで登っている間に、山頂まで登った。', '太郎は花子が山頂まで登る前に、山頂まで登った。')]\n",
    "y_test = ['entailment']\n",
    "features_test, labels_test = convert_examples_to_features(x_test, y_test, target_vocab, max_seq_length=maxlen, tokenizer=tokenizer)\n",
    "\n",
    "# ラベルを予測\n",
    "label_ids = model.predict(features_test)\n",
    "label_ids = np.argmax(label_ids, axis=-1)\n",
    "y_pred = target_vocab.decode(label_ids)\n",
    "y_true = target_vocab.decode(labels_test)\n",
    "print(y_pred, y_true)\n",
    "\n",
    "# 混同行列の作成\n",
    "evaluate(model, target_vocab, features_test, labels_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPUZlUQqKOOS5lcB8JL0TDt",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "JapaneseXLM_NLI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
